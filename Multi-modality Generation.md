
### Multi-modality Generation 

### [Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)


[arxiv 2023.07]Generative Pretraining in Multimodality [[PDF](https://arxiv.org/abs/2307.05222),[Page](https://github.com/baaivision/Emu)]

[arxiv 2023.07]Generating Images with Multimodal Language Models [[PDF](https://arxiv.org/abs/2305.17216),[Page](https://jykoh.com/gill)]

[arxiv 2023.07]3D-LLM: Injecting the 3D World into Large Language Models[[PDF] (https://arxiv.org/abs/2307.12981),[Page](https://vis-www.cs.umass.edu/3dllm/)]

[arxiv 2023.10]Making LLaMA SEE and Draw with SEED Tokenizer [[PDF](https://arxiv.org/abs/2310.01218),[Page](https://github.com/AILab-CVC/SEED)]

[arxiv 2023.10]Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation [[PDf](https://arxiv.org/abs/2310.08541),[Page](https://idea2img.github.io/)]

[arxiv 2023.12]CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation[[PDF](https://arxiv.org/abs/2311.18775),[Page](https://codi-2.github.io/)]

[arxiv 2023.12]SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models [[PDF](https://arxiv.org/abs/2312.06739),[Page](https://yuzhou914.github.io/SmartEdit/)]

[arxiv 2023.12]InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following [[PDF](https://arxiv.org/abs/2312.06738), [Page](https://github.com/jacklishufan/InstructAny2Pix.git)]

[arxiv 2023.12]Massively Multimodal Masked Modeling [[PDF](https://arxiv.org/abs/2312.06647),[Page](https://4m.epfl.ch/)]

[arxiv 2023.12]Gemini: A Family of Highly Capable Multimodal Models [[PDF](https://arxiv.org/abs/2312.11805)]

[arxiv 2023.12]Generative Multimodal Models are In-Context Learners [[PDF](https://arxiv.org/abs/2312.13286),[Page](https://baaivision.github.io/emu2)]

[arxiv 2024.1]DiffusionGPT: LLM-Driven Text-to-Image Generation System [[PDF](https://arxiv.org/abs/2401.10061)]

[arxiv 2024.1]Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation [[PDF](https://arxiv.org/abs/2401.17664),[Page](https://vlislab22.github.io/ImageAnything/)]

[arxiv 2024.03]3D-VLA 3D Vision-Language-Action Generative World Model[[PDF](https://arxiv.org/abs/2403.09631),[Page](https://vis-www.cs.umass.edu/3dvla/)]

[arxiv 2024.03]LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models [[PDF](https://arxiv.org/abs/2403.15388),[Page](https://llava-prumerge.github.io/)]

[arxiv 2024.04]SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation [[PDF](https://arxiv.org/abs/2404.14396)]

[arxiv 2024.06]The Evolution of Multimodal Model Architectures [[PDF](https://arxiv.org/pdf/2405.17927)]

[arxiv 2024.08] Show-o: One Single Transformer to Unify Multimodal Understanding and Generation[[PDF](https://arxiv.org/abs/2408.12528), [Page](https://github.com/showlab/Show-o)]

[arxiv 2024.09] VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation[[PDF](https://arxiv.org/abs/2409.04429)]

[arxiv 2024.09]EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions[[PDF](https://arxiv.org/abs/2409.18042), [Page](https://emova-ollm.github.io/)]


[arxiv 2024.09] Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models[[PDF](https://arxiv.org/abs/2409.17146),)]


[arxiv 2024.09]MonoFormer: One Transformer for Both Diffusion and Autoregression [[PDF](https://arxiv.org/abs/2409.16280), [Page](https://monoformer.github.io/)]


[arxiv 2024.09] Visual Prompting in Multimodal Large Language Models: A Survey[[PDF](https://arxiv.org/abs/2409.15310)]




[arxiv 2024.09] [[PDF](), [Page]()]
